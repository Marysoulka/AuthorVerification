{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqx2FHkjYplC",
        "outputId": "a323ec6f-b949-4450-ec3c-a16568b0bcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Conv1D, MaxPooling1D, Lambda, LSTM, Dropout, BatchNormalization, Activation\n",
        "import tensorflow as tf  # Import TensorFlow to use math functions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2gmAgsEaCjd",
        "outputId": "f508625a-ebc3-4363-a51b-9e016db4f31b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## •\tLoading the Dataset"
      ],
      "metadata": {
        "id": "0ulpjwdu1JTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/My Drive/Gungor_2018_VictorianAuthorAttribution_data-train.csv'\n",
        "data = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "Mn7aTpWqaGqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15773461-a523-451a-c8ba-bd0ae4468486"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  author\n",
            "0  ou have time to listen i will give you the ent...       1\n",
            "1  wish for solitude he was twenty years of age a...       1\n",
            "2  and the skirt blew in perfect freedom about th...       1\n",
            "3  of san and the rows of shops opposite impresse...       1\n",
            "4  an hour s walk was as tiresome as three in a s...       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step involves preparing text data for the Siamese Neural Network by tokenizing it at the character level and creating pairs of text samples labeled as \"similar\" or \"dissimilar.\" Positive pairs (texts by the same author) are labeled as 1, while negative pairs (texts by different authors) are labeled as 0. Texts are converted into numerical sequences, padded to a uniform length, and stored as input pairs with labels, ensuring a balanced dataset for effective training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7lA7JmLt1DtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text at the character level\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(data['text'])\n",
        "def create_pairs(data, tokenizer, max_length=800, num_negative=1, max_pairs_per_author=10, max_authors=10):\n",
        "    \"\"\"\n",
        "    Create positive and negative pairs with optimizations.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    authors = data.groupby('author')['text'].apply(list).to_dict()\n",
        "    author_list = list(authors.keys())[:max_authors]  # Limit to a subset of authors\n",
        "\n",
        "    for author_index, author in enumerate(author_list):\n",
        "        print(f\"Processing author {author_index + 1}/{len(author_list)}: {author}\")\n",
        "\n",
        "        # Limit the number of texts per author\n",
        "        texts = authors[author][:max_pairs_per_author]\n",
        "\n",
        "        # Positive pairs\n",
        "        for i in range(len(texts)):\n",
        "            for j in range(i + 1, len(texts)):\n",
        "                seq1 = tokenizer.texts_to_sequences([texts[i]])[0]\n",
        "                seq2 = tokenizer.texts_to_sequences([texts[j]])[0]\n",
        "                seq1 = pad_sequences([seq1], maxlen=max_length)[0]\n",
        "                seq2 = pad_sequences([seq2], maxlen=max_length)[0]\n",
        "                pairs.append(np.hstack((seq1, seq2)))\n",
        "                labels.append(1)\n",
        "\n",
        "        # Negative pairs\n",
        "        for text in texts:\n",
        "            for _ in range(num_negative):\n",
        "                other_author = random.choice([a for a in author_list if a != author])\n",
        "                other_text = random.choice(authors[other_author])\n",
        "                seq1 = tokenizer.texts_to_sequences([text])[0]\n",
        "                seq2 = tokenizer.texts_to_sequences([other_text])[0]\n",
        "                seq1 = pad_sequences([seq1], maxlen=max_length)[0]\n",
        "                seq2 = pad_sequences([seq2], maxlen=max_length)[0]\n",
        "                pairs.append(np.hstack((seq1, seq2)))\n",
        "                labels.append(0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)"
      ],
      "metadata": {
        "id": "Y86gsWn6aqVx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was split into 80% for training and 20% for testing using the `train_test_split` function with a fixed random state for consistent results. From the training and testing data, pairs of texts were created using the `create_pairs` function. A maximum of 10 authors were selected for training, with up to 20 text pairs generated per author. The output included paired sequences (`Xtrain` and `Xtest`) and their binary labels (`Ytrain` and `Ytest`), indicating whether the paired texts were written by the same author or different authors."
      ],
      "metadata": {
        "id": "NC3ibMVm2mrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "Xtrain, Ytrain = create_pairs(train_data, tokenizer, max_authors=10, max_pairs_per_author=20)\n",
        "Xtest, Ytest = create_pairs(test_data, tokenizer, max_authors=10, max_pairs_per_author=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8ljWa3ravwU",
        "outputId": "19a5da03-69e2-4e45-b0df-2a9febbc4e52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing author 1/10: 1\n",
            "Processing author 2/10: 2\n",
            "Processing author 3/10: 3\n",
            "Processing author 4/10: 4\n",
            "Processing author 5/10: 6\n",
            "Processing author 6/10: 8\n",
            "Processing author 7/10: 9\n",
            "Processing author 8/10: 10\n",
            "Processing author 9/10: 11\n",
            "Processing author 10/10: 12\n",
            "Processing author 1/10: 1\n",
            "Processing author 2/10: 2\n",
            "Processing author 3/10: 3\n",
            "Processing author 4/10: 4\n",
            "Processing author 5/10: 6\n",
            "Processing author 6/10: 8\n",
            "Processing author 7/10: 9\n",
            "Processing author 8/10: 10\n",
            "Processing author 9/10: 11\n",
            "Processing author 10/10: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the data as .npy files\n",
        "np.save('Xtrain.npy', Xtrain)\n",
        "np.save('Ytrain.npy', Ytrain)\n",
        "np.save('Xtest.npy', Xtest)\n",
        "np.save('Ytest.npy', Ytest)\n",
        "\n",
        "print(\"Data preprocessing complete. Files saved as Xtrain.npy, Ytrain.npy, Xtest.npy, Ytest.npy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhdH6gWpazcE",
        "outputId": "a67cb176-5e10-4fd1-9ff6-becdb6ee1b86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete. Files saved as Xtrain.npy, Ytrain.npy, Xtest.npy, Ytest.npy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LoadData` function loads the training and testing data, splits each input pair into two parts (left and right), and prepares it for the Siamese Neural Network. It also reshapes the data if needed and calculates the size of the inputs to ensure they fit the model. The function returns the processed data and labels, ready for training and testing.\n"
      ],
      "metadata": {
        "id": "n7vIUyZw3RWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadData(path_Xtrain, path_Ytrain, path_xtest, path_ytest):\n",
        "    # Load data\n",
        "    Xtrain = np.load(path_Xtrain)\n",
        "    Ytrain = np.load(path_Ytrain)\n",
        "    Xtest = np.load(path_xtest)\n",
        "    Ytest = np.load(path_ytest)\n",
        "\n",
        "    # Reshape data to add a channel dimension if it's missing\n",
        "    Xtrain = Xtrain.reshape((Xtrain.shape[0], Xtrain.shape[1], 1))\n",
        "    Xtest = Xtest.reshape((Xtest.shape[0], Xtest.shape[1], 1))\n",
        "\n",
        "    # Split input vectors into two parts\n",
        "    XtrainLeft = Xtrain[:, 0:800, :]\n",
        "    XtrainRigth = Xtrain[:, 800:1600, :]\n",
        "    XtestLeft = Xtest[:, 0:800, :]\n",
        "    XtestRigth = Xtest[:, 800:1600, :]\n",
        "\n",
        "    longitud = XtrainLeft.shape[1]\n",
        "    dimension = XtrainLeft.shape[2]\n",
        "\n",
        "    return XtrainLeft, XtrainRigth, Ytrain, XtestLeft, XtestRigth, Ytest, longitud, dimension"
      ],
      "metadata": {
        "id": "Eh9Rtuuxh3ne"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this architecture is designed to extract features from input sequences and represent them in a compact form for similarity comparison. It balances feature extraction (via convolutional layers) with sequential learning (via the LSTM layer), making it effective for tasks like authorship verification."
      ],
      "metadata": {
        "id": "Ke25zclG3hNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SiameseArquitecture(longitud, dimension):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv1D(75, 12, input_shape=(longitud, dimension)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Activation('relu'))\n",
        "\tmodel.add(Dropout(0.1))\n",
        "\tmodel.add(Conv1D(50, 12))\n",
        "\tmodel.add(Activation('relu'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.1))\n",
        "\tmodel.add(MaxPooling1D(4))\n",
        "\tmodel.add(LSTM(64, recurrent_dropout=0.1, return_sequences=False))\n",
        "\tmodel.add(Activation('relu'))\n",
        "\n",
        "\tmodel.summary()\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "gGQDf60vh5Xx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is used to measure the similarity between two feature vectors in the Siamese Neural Network. A smaller distance indicates higher similarity, while a larger distance implies dissimilarity."
      ],
      "metadata": {
        "id": "pGWsQOqc3tNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return tf.math.sqrt(tf.math.maximum(tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True), K.epsilon()))"
      ],
      "metadata": {
        "id": "gWTylJeUiBix"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function defines the shape of the output for the Euclidean distance layer in a Siamese Neural Network"
      ],
      "metadata": {
        "id": "9dhh2gkJ4EwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eucl_dist_output_shape(shapes):\n",
        "\tshape1, shape2 = shapes\n",
        "\treturn (shape1[0], 1)"
      ],
      "metadata": {
        "id": "S_Em9MX1iEFc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function measures the network's ability to differentiate similar and dissimilar pairs effectively(based on a distance threshold of 0.5).It's a simple and effective way to evaluate the model's performance during training and testing."
      ],
      "metadata": {
        "id": "blhzItcd4Xb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(predictions, labels):\n",
        "\treturn labels[predictions.ravel() < 0.5].mean()"
      ],
      "metadata": {
        "id": "k-eVLlAEiG2F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section sets up a Siamese Neural Network. It loads training and testing data from specified file paths, splits the inputs into two parts, and processes them through a shared network. The Euclidean distance is calculated to measure how similar the two inputs are. The model is then compiled with a loss function and optimizer, preparing it for training."
      ],
      "metadata": {
        "id": "4JsXEd8X8NjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"-X\", \"--path_Xtrain\", help=\"Path X train\")\n",
        "parser.add_argument(\"-Y\", \"--path_Ytrain\", help=\"Path Y train\")\n",
        "parser.add_argument(\"-x\", \"--path_xtest\", help=\"Path x test\")\n",
        "parser.add_argument(\"-y\", \"--path_ytest\", help=\"Path y test\")\n",
        "\n",
        "\n",
        "path_Xtrain = 'Xtrain.npy'\n",
        "path_Ytrain = 'Ytrain.npy'\n",
        "path_xtest = 'Xtest.npy'\n",
        "path_ytest = 'Ytest.npy'\n",
        "\n",
        "np.random.seed(9)\n",
        "XtrainLeft, XtrainRigth, Ytrain, XtestLeft, XtestRigth, Ytest, longitud, dimension = LoadData(path_Xtrain, path_Ytrain, path_xtest, path_ytest)\n",
        "\n",
        "\n",
        "Siamese = SiameseArquitecture(longitud, dimension)\n",
        "input1 = Input(shape=(longitud,dimension))\n",
        "input2 = Input(shape=(longitud,dimension))\n",
        "\n",
        "brenchLeft = Siamese(input1)\n",
        "brenchRight = Siamese(input2)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([brenchLeft, brenchRight])\n",
        "\n",
        "rms = RMSprop()\n",
        "model = Model([input1,input2], distance)\n",
        "model.compile(loss='mean_squared_error', optimizer=rms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "GeNjN8MGiLqC",
        "outputId": "779e67ea-5ec2-4c5b-c818-1a6dcd4c1d77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m789\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │             \u001b[38;5;34m975\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m789\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │             \u001b[38;5;34m300\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m789\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m789\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m778\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │          \u001b[38;5;34m45,050\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m778\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m778\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m778\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m194\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m29,440\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">789</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">975</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">789</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">789</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">789</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">778</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">45,050</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">778</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">778</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">778</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">194</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">29,440</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m75,965\u001b[0m (296.74 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">75,965</span> (296.74 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m75,715\u001b[0m (295.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">75,715</span> (295.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m250\u001b[0m (1000.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">250</span> (1000.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section trains the Siamese Neural Network for 25 epochs, evaluating its performance after each epoch. It tracks training and testing accuracy using the `compute_accuracy` function and monitors progress through printed results. After completing the training, the model is saved to a file named `lstm_model.h5` for future use."
      ],
      "metadata": {
        "id": "QMJfXJ1K8b5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracc, tsacc = [], []\n",
        "trloss, tsloss = [], []\n",
        "# Loop to evaluate 25 epochs\n",
        "for i in range(25):\n",
        "\tprint(\"->Epoch: \", i+1)\n",
        "\thistory = model.fit([XtrainLeft, XtrainRigth], Ytrain,\n",
        "\tvalidation_data=([XtestLeft, XtestRigth],Ytest), epochs=1, batch_size=512)\n",
        "\tpred = model.predict([XtrainLeft, XtrainRigth])\n",
        "\ttr_acc = compute_accuracy(pred, Ytrain)\n",
        "\tpred = model.predict([XtestLeft, XtestRigth])\n",
        "\tte_acc = compute_accuracy(pred, Ytest)\n",
        "\tprint(\"Train acc: \", tr_acc)\n",
        "\tprint(\"Test acc: \", te_acc)\n",
        "\n",
        "model.save('lstm_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxJWQ27fiRmh",
        "outputId": "d5952f82-446b-4295-bbbb-74847d7433a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "->Epoch:  1\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 12s/step - loss: 0.1574 - val_loss: 0.3315\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 170ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 174ms/step\n",
            "Train acc:  0.9034021871202916\n",
            "Test acc:  0.9040047114252061\n",
            "->Epoch:  2\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12s/step - loss: 0.0997 - val_loss: 0.3061\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 175ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step\n",
            "Train acc:  0.897208985704561\n",
            "Test acc:  0.9050387596899225\n",
            "->Epoch:  3\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12s/step - loss: 0.0941 - val_loss: 0.2953\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 171ms/step\n",
            "Train acc:  0.884643644379133\n",
            "Test acc:  0.9079552925706772\n",
            "->Epoch:  4\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 13s/step - loss: 0.0921 - val_loss: 0.2703\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step\n",
            "Train acc:  0.8777885548011639\n",
            "Test acc:  0.90056134723336\n",
            "->Epoch:  5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0879 - val_loss: 0.2625\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 156ms/step\n",
            "Train acc:  0.8602711157455682\n",
            "Test acc:  0.9046413502109705\n",
            "->Epoch:  6\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0889 - val_loss: 0.2613\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step\n",
            "Train acc:  0.8349282296650717\n",
            "Test acc:  0.9035836177474402\n",
            "->Epoch:  7\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 13s/step - loss: 0.0826 - val_loss: 0.2427\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 175ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step\n",
            "Train acc:  0.7855887521968365\n",
            "Test acc:  0.9058823529411765\n",
            "->Epoch:  8\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13s/step - loss: 0.0843 - val_loss: 0.2288\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step\n",
            "Train acc:  0.7575107296137339\n",
            "Test acc:  0.8921438082556591\n",
            "->Epoch:  9\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0809 - val_loss: 0.2486\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "Train acc:  0.7809983896940419\n",
            "Test acc:  0.903096903096903\n",
            "->Epoch:  10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 13s/step - loss: 0.0851 - val_loss: 0.1819\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 169ms/step\n",
            "Train acc:  0.5416666666666666\n",
            "Test acc:  0.9087719298245615\n",
            "->Epoch:  11\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0789 - val_loss: 0.2112\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 142ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step\n",
            "Train acc:  0.5800865800865801\n",
            "Test acc:  0.8902439024390244\n",
            "->Epoch:  12\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0795 - val_loss: 0.1765\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step\n",
            "Train acc:  0.375\n",
            "Test acc:  0.9285714285714286\n",
            "->Epoch:  13\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0741 - val_loss: 0.1888\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 170ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 147ms/step\n",
            "Train acc:  0.36153846153846153\n",
            "Test acc:  0.9156626506024096\n",
            "->Epoch:  14\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0766 - val_loss: 0.1648\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 158ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "Train acc:  0.20833333333333334\n",
            "Test acc:  0.896551724137931\n",
            "->Epoch:  15\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12s/step - loss: 0.0781 - val_loss: 0.1679\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 217ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step\n",
            "Train acc:  0.2125\n",
            "Test acc:  0.9333333333333333\n",
            "->Epoch:  16\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 12s/step - loss: 0.0669 - val_loss: 0.1542\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 177ms/step\n",
            "Train acc:  0.10638297872340426\n",
            "Test acc:  0.953125\n",
            "->Epoch:  17\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12s/step - loss: 0.0695 - val_loss: 0.1699\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 155ms/step\n",
            "Train acc:  0.05263157894736842\n",
            "Test acc:  0.9381443298969072\n",
            "->Epoch:  18\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12s/step - loss: 0.0662 - val_loss: 0.1607\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 175ms/step\n",
            "Train acc:  0.056338028169014086\n",
            "Test acc:  0.9271523178807947\n",
            "->Epoch:  19\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12s/step - loss: 0.0692 - val_loss: 0.1472\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 167ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 181ms/step\n",
            "Train acc:  0.05357142857142857\n",
            "Test acc:  0.9292929292929293\n",
            "->Epoch:  20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12s/step - loss: 0.0674 - val_loss: 0.1369\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 160ms/step\n",
            "Train acc:  0.1111111111111111\n",
            "Test acc:  0.9322033898305084\n",
            "->Epoch:  21\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12s/step - loss: 0.0687 - val_loss: 0.1454\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 174ms/step\n",
            "Train acc:  0.04285714285714286\n",
            "Test acc:  0.9195402298850575\n",
            "->Epoch:  22\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 12s/step - loss: 0.0637 - val_loss: 0.1387\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "Train acc:  0.03225806451612903\n",
            "Test acc:  0.9333333333333333\n",
            "->Epoch:  23\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 12s/step - loss: 0.0631 - val_loss: 0.1483\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 179ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 163ms/step\n",
            "Train acc:  0.04878048780487805\n",
            "Test acc:  0.9056603773584906\n",
            "->Epoch:  24\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12s/step - loss: 0.0619 - val_loss: 0.1331\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step\n",
            "Train acc:  0.04081632653061224\n",
            "Test acc:  0.926829268292683\n",
            "->Epoch:  25\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 12s/step - loss: 0.0600 - val_loss: 0.1416\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 178ms/step\n",
            "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 169ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc:  0.013513513513513514\n",
            "Test acc:  0.8928571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Texts for testing\n",
        "text1 = \"to fall gently from a small hotel he saw a couple and heard a hasty good night exchanged as they separated at the doorway in opposite directions the man who was dressed whistled for a passing cab and was driven away leaving his companion alone the woman walked slowly looking about as if expecting some one and presently a second man came out of the shadow of a building and walked up to her how much he asked holding out his hand four pounds was the answer in a disappointed voice i told you to insist on five said the man angrily he would not give it i did the best i could what an idiot you are was the response as the speaker took the money well i ve no time to lose i ll see you in the morning he was turning away apparently in great haste when the woman spoke again am i to walk home i haven t a shilling with an expression that sounded like a curse the man took a piece of silver from his pocket and a blunt refusal it into her hand the movement was so abrupt that the woman staggered with the force of it then he left her gray could not doubt the full significance of the proceeding he had witnessed the man was one of those creatures who live on the of women and he had taken from his mistress the for which she had just sacrificed her and her honor she had given it to him not unwillingly only making a slight protest when he left her to walk through the streets at one o clock at night for want of a cab fare it was a proceeding gray had heard of before but of which ha had always entertained some doubt stunned by the scene he was walking on in a sort of when the woman who had no suspicion that her recent actions had been noticed by him hastened on his track it is a cold night sir she said he walked faster but she caught up with him and for nearly a minute into his ears invitations that filled him with horror at last he saw a policeman in the distance and stopping short threatened to give her into if she did not leave him at once the startled look of the woman when he uttered these words made him sorry that he had spoken so sharply he drew out a half sovereign though he had little enough to spare and told her to take it she reached for the money with an that was astonishing and only to be accounted for by hunger or some equally strong reason then with a searching look at his face she vanished up one of the streets love gone astray and gray went back to his temporary lodging wondering if he could trust his eyes and his ears for the man he had seen lying in wait for this poor creature and taking her shameful to the last penny was none other than the one he had rescued in and encountered again in rome the elegant rider he so recently met in the park his old acquaintance chapter ix shall it be you or he f nature had its way with him before lie was aware of it and he fell into a deep sleep when he awoke it was past eight o clock and the room was brightly illuminated with the wintry sun he rubbed his eyes as he realized what must have occurred and was about to go to the street to summon a cab when he heard a familiar voice in the outer entry how early do you expect mr asked the voice about half past nine was the reply of the domestic addressed i must see him sooner said the first voice give me his house address he never business at his house said the domestic and i have orders to give the address to no one shall it be you or he there was an angry and impatient exclamation at this i shall find it in some way said the stranger if i do not i shall be here again within an hour would the gentleman leave his card no the gentleman would not the gentleman in an ill temper and in great haste for he went out with a of the door that shook the building mr gray had risen from the chair in which he slept and stood staring in the direction from which these sounds proceeded the voice he had heard was a familiar one good heaven how could that man appear at every turn in his path it was tolerably clear was one of those eligible young gentlemen whom mr had selected as possible partners for his fair such a fate for the poor girl was too horrible gray seized a sheet of note paper and wrote rapidly dear mr see me without fail before you make any arrangement in the matter of which we were speaking i shall wait in my room till you come do not fail to heed this yours g g take that as fast as you can ride to mr he said when the domestic answered his bell put it into his hands yourself it was only half an hour before the domestic returned bringing the message that mr would be at his office as soon as possible love gone and fretted however as the time dragged on he had come to feel that the of a crime lay in his hands the character of had presented itself to him so that he could not endure the thought that a young girl of the sort the had described of her one fault should be condemned to such a life it would be even better to endure the pangs of to face the cruel world with her guilt exposed than to marry such a man  \"  # Author 1\n",
        "text2 = \"she thought feathers the only proper thing what a wretch he was to make her come into that cold room when she was not used to it just to prove that he could have his way would \"  # Author 1\n",
        "text3 = \"but it was more and his progress through it was more laborious the wind too which came roaring down from old in the over the broad open surface of the pond made it very hard for him to struggle on he succeeded however at length in fairly gaining the opposite shore without actually losing his track and then after a short walk in a sheltered valley he turned out of the road into the doctor s yard and up to his door and now since he is safely there we will return to george and mary george went back to his shop to finish george and mary in his cradle mary s great anxiety his work promising to return again then and take care of while mary prepared supper he accordingly came in again after half an hour looking up anxiously as he crossed the yard at the signs of increasing violence in the storm as he entered mary was rocking and he came and took her place he had made himself a expressly for the purpose of rocking it was like any ordinary chair except in height the seat being only about eight inches from the floor to accommodate it to his stature he drew this chair up to the side of the cradle said he looking into the cradle and holding out his hands to the little sufferer want to come and rock with father made an effort to reach out his hands but from weakness they dropped back again at his side george took him up gently and laying he child s face upon his shoulder murmured words of sympathy and in his ear mary went to the window oh my george said she what a storm poor he never will get across the pond how could we let him go we did the best we could mary and now you must not make yourself and me anxious and unhappy about it why how can i help feeling anxious said she my poor boy out on a lone road in such a storm as this and night coming on we can help feeling anxious in a measure replied george we can try to think of something else and if an anxious thought comes into your mind don t speak it out speaking it out makes it stronger the child is in god s hands and we have now nothing to do for him mary could not reply to this and she went about her b submission mary makes a torn over work preparing supper but her mind was ill at ease she could not deny george s position that their boy was entirely out of their hands and that god by making it plainly their duty to send him at least as it appeared to them had taken the responsibility of his safety into his own hands but yet after all her heart was not she could not let him go and feel that she had no to do but to await the decision of another george too felt an instinctive parental solicitude which made him follow in imagination every step of s way but his heart was subdued and to the will of god in regard to the result so that he was calm and peaceful in spirit though the swelling emotions of his heart repeatedly filled his eyes he in s ear in words too imperfectly to be heard the good old hymn lift mine eyes from god is all my aid and they who know by experience what it is really to resign every thing into god s hands in an hour of serious danger or trouble will not think it strange that he spent half an hour in a state of very pure and enjoyment in the mean time mary was busy in her preparations for supper and particularly in making a little apple turn over for against he came back an apple turn over was s highest idea of luxury and mary by her interest in making it got over another half hour very well the time however soon arrived when she began to listen for the doctor s bells she began to listen for them a full quarter of an hour before they could have been reasonably expected but this quarter of an hour glided away very soon and the daylight began sensibly to decline she left her work repeatedly to go to the window and look out mary becomes impatient submission impossible george anxiously at last she asked george if it was not time for them to come why no said george hesitating i should hardly expect them yet it is two hours and more already and it is growing dark mary brought her face close to the glass her eyes from the light in the room by putting her hands upon each side of them and straining her sight to look down the road but the snow which filled the air and drove against the window and down on the outside prevented her seeing much i do not believe it is possible for the poor little fellow to get across the pond in such a night as this well mary we have nothing to do but to wait quietly for the end now there is nothing we can do and it is wrong to be restless and anxious about it oh dear said mary sitting down and gazing into the fire with a look of great distress how sorry i am we let him go might have gone and now he will perish in the snow and i shall never have another moment s peace as long as i live but consider mary said george we have done the best we could and he is in god s hands you are not willing to leave him there oh george said she it is too dreadful she rose and walked back and forth across the room with a hurried and \"  # Author 2"
      ],
      "metadata": {
        "id": "YqUxhVkb_kC8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_author_similarity(model, tokenizer, text1, text2, max_length=800):\n",
        "    \"\"\"\n",
        "    Predict if two given texts are written by the same author.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        tokenizer: Tokenizer used during training.\n",
        "        text1 (str): First text input.\n",
        "        text2 (str): Second text input.\n",
        "        max_length (int): Maximum sequence length for padding.\n",
        "\n",
        "    Returns:\n",
        "        str: \"Same author\" or \"Different authors\".\n",
        "    \"\"\"\n",
        "    # Tokenize and pad the texts\n",
        "    seq1 = tokenizer.texts_to_sequences([text1])[0]\n",
        "    seq2 = tokenizer.texts_to_sequences([text2])[0]\n",
        "    seq1 = pad_sequences([seq1], maxlen=max_length)\n",
        "    seq2 = pad_sequences([seq2], maxlen=max_length)\n",
        "\n",
        "    # Predict using the two inputs separately\n",
        "    prediction = model.predict([seq1, seq2], verbose=0)[0][0]  # Get the prediction (distance)\n",
        "\n",
        "\n",
        "    print(f\"Prediction : {prediction}\")\n",
        "    if prediction > 0.5:\n",
        "        return \"Same author\"\n",
        "    else:\n",
        "        return \"Different authors\"\n"
      ],
      "metadata": {
        "id": "1Dgt6k59PrwC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text1 vs Text2:\", predict_author_similarity(model, tokenizer, text1, text2))\n",
        "print(\"Text1 vs Text3:\", predict_author_similarity(model, tokenizer, text1, text3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyaUG28VQfzr",
        "outputId": "64df49c9-c570-4917-c012-e8be79064ef4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction : 0.5954779982566833\n",
            "Text1 vs Text2: Same author\n",
            "Prediction : 0.47985658049583435\n",
            "Text1 vs Text3: Different authors\n"
          ]
        }
      ]
    }
  ]
}